\section{Compared regression techniques}
\label{sec:techs}

In the prediction of the development rate of the black Sigatoka, we
compare techniques such as least squares regression and elastic-net regression, commonly
encountered in the agricultural literature with machine learning
methods such as support vector regression and echo
state networks, where the parameter space of each technique is also
taken into account.

\subsection{Ordinary least squares regression}

Given a data set 
\begin{equation}
  D=\left\{(\vct{x}_i,y_i) \mid i=1\ldots n\right\}
\end{equation}
composed of the $d$-dimensional%
\footnote{Without loss of generality assume that the first component
  of every vector $\vct{x}_i$ is always $1$.}
%
feature vectors $\vct{x}_i\in\setR^d$ and the corresponding responses
$y_i$.
%
The ordinary least squares regression (OLSR) fits a linear model
$\tilde{y}_i = f(\vct{x}_i) = \dotp{\vct{w},\vct{x}_i}$ such that the sum of
squares of the residuals $(\tilde{y}_i-y_i)$ is minimized.
%
Let $\mat{X}$ be the $n\times{}d$ feature matrix containing the $i$-th
data sample $\vct{x}_i^T$ in its $i$-th row and $\vct{y}$ contain all
the responses $y_i$ corresponding to each row, then the least squares
regression finds
\begin{equation}
\label{eq:problem}
  \hat{\vct{w}} =
  \arg\min_{\vct{w}} E(\vct{w})
\end{equation}
with the error function
\begin{equation*}
  E(\vct{w}) =  \left\| \mat{X}\vct{w} - \vct{y}\right\|_2^2
\end{equation*}
The solution is found by means of the pseudoinverse
$\hat{\vct{w}}=\mat{X}^+\vct{y}=(\mat{X}^T\mat{X})^{-1}\mat{X}^T\vct{y}$
or equivalently by the singular value decomposition of $\mat{X}$
\citep{Press2007}.

\subsection{Elastic net regression}

Instead of $L_2$ regularization prior
($\alpha\left\|\vct{w}\right\|_2^2$) included in the ridge regression (RR),
\citet{Tibshirani1996} used an $L_1$ term ($\lambda
\left\|\vct{w}\right\|_1$) for his lasso estimator, which permits to
select a subset of the available features by zeroing the weights of
the deselected features.
%
If the dimension $d$ of the data is larger than the number $n$ of data
samples, lasso will select a maximum of $d$ variables.

The elastic net regression (ENR) of \citet{Zou2005} combines both
$L_1$ and $L_2$ priors of the ridge and lasso estimators such that the
error function is now
\begin{equation*}
  E(\vct{w}) = 
  \left\| \mat{X}\vct{w} - \vct{y} \right\|_2^2 + 
  \alpha \left\|\vct{w}\right\|_2^2 +
  \lambda \left\|\vct{w}\right\|_1 
\end{equation*}
%
This combination of priors still allows to learn a sparse model with only a 
few weights being non-zero like in the case of lasso, but still
maintaining the regularization properties of the ridge regression
\citep{scikitlearn2011}.

The elastic net is useful when multiple features are correlated: lasso
will likely pick one of these at random, while the elastic net will
still likely pick both.

\subsection{Support Vector Regression (SVR)}

From the perspective of Support Vector Regression (SVR) the regression
function is usually formulated as 
\begin{equation}
\label{eq:svrfunc}
  \tilde{y} = f(\vct{x}) = \dotp{\vct{w},\vct{x}} + b
\end{equation}
The weights are selected in a convex optimization problem
\citep{Smola2004}:
\begin{align*}
  \text{minimize} \quad  & \frac{1}{2}\|\vct{w}\|^2 + C\sum_{i=1}^n(\xi_i+\xi_i^\ast) \\
  \text{subject to} \quad & 
  \begin{cases}
    y_i - \dotp{\vct{w},\vct{x}_i} - b &\leq \epsilon + \xi_i    \\
    \dotp{\vct{w},\vct{x}_i} + b -y_i  &\leq \epsilon + \xi_i^\ast\\
    \xi_i,\xi_i^\ast               &\geq 0
  \end{cases}
\end{align*}
where $\epsilon$ is the maximal allowed deviation of the targets
$\tilde{y}_i$ from the responses $y_i$, the slack variables $\xi_i$
and $\xi_i^\ast$ allow to cope with otherwise unfeasible constraints
for the optimization problem, and the constant $C>0$ controls the
trade-off between the flatness of $f$ and the tolerance to deviations
larger than $\epsilon$.

Note that since OLSR, RR and ENR use a squared error function, data
outliers will have a strong influence on the resulting weights
$\vct{w}$.  On the SVR formulation, however, the usage of the $L_1$
norm and the slack variables considerably restrict or completely block
the influence of those outliers.

The SVR problem is reformulated by means of the dual optimization
problem into \citep{Smola2004}
\begin{equation}
  \label{eq:svrw}
  \vct{w}=\sum_{i=1}^n(\alpha_i-\alpha_i^\ast)\vct{x}_i
\end{equation}
where $\alpha_i,\alpha_i^\ast\in[0,C]$ are Lagrange multipliers
subject to $\sum_{i=1}^n(\alpha_i-\alpha_i^\ast)=0$.
%
In this so-called \emph{Support Vector expansion} the weights are
expressed as a linear combination of the data set patterns
$\vct{x}_i$.
%
Inserting (\ref{eq:svrw}) in (\ref{eq:svrfunc}) leads to
\begin{equation}
  \label{eq:svrf}
 f(\vct{x}) = \sum_{i=1}^n(\alpha_i-\alpha_i^\ast)\dotp{\vct{x}_i,\vct{x}} + b
\end{equation}
The Lagrange multipliers $\alpha_i,\alpha_i^\ast$ are both non-zero
only for those data points where $|f(\vct{x}_i) - y_i|\geq\epsilon$.
Hence, the expansion of $\vct{w}$ in terms of $\vct{x}_i$ is sparse.
Those data points with non-vanishing coefficients are called
\emph{Support Vectors} \citep{Wei2013}.

Additionally, in (\ref{eq:svrf}) it is possible to employ the
\emph{kernel trick} and replace the terms $\dotp{\vct{x}_i,\vct{x}}$ with
the evaluation of any Mercer kernel
$k(\vct{x}_i,\vct{x})=\dotp{\phi(\vct{x}_i),\phi(\vct{x})}$, where
$\phi(\vct{x})$ is a non-linear mapping of the input space onto a
higher (even infinite) dimensional feature space.
%
The kernel evaluation draws unnecessary the explicit evaluation of the
non-linear mapping, and it allows to solve non-linear regressions in
the input space by implicitly mapping the samples through the kernel
into the higher dimensional space, where the linear regression occurs
\citep{Alonso2013}.

Kernels used in this work were:

Linear kernel: 
\begin{equation*}
K(\vct{x}_i,\vct{x}_j)={\vct{x}_i}^T \vct{x}_j
\end{equation*}

Radial basis function (RBF): 
\begin{equation*}
K(\vct{x}_i,\vct{x}_j) = exp\left( \frac{ \| \vct{x}_i - \vct{x}_j \|^2}{2 \sigma^2} \right)
\end{equation*}
where $\sigma$ is the parameter of gausian model.

Sigmoid:
\begin{equation*}
K(\vct{x}_i,\vct{x}_j) = tanh \left[ -c + \frac{ \vct{x}_i  \vct{x}_j }{ \sigma^2} \right]
\end{equation*}
with $c \geq 0$  and $\sigma^2$ is the scaling vector.

\subsection{Echo State Networks (ESN)}

Recurrent neural networks (RNN) are capable of learning temporal
patterns by feeding neuron outputs back into lower layers. Their
training (usually by means of error backpropagation) is in general
slow.
%
Echo state networks (ESN) are a particular type of recurrent neural
network with a sparsely connected random hidden layer where only the
weights of the output neurons are changed at training.  The randomly
selected weights at the input and middle layers (called \emph{reservoir})
reproduce temporal patterns (\emph{echoes}) that the output layer
learns to select during the training \citep{Lukose2009}.

For a given training input signal $u(n) \in \setR^{N_u}$ a desired target
output signal $y^{target}(n) \in \setR^{N_y}$ is known.
%
Here $n = 1,\ldots,T$ is the discrete time and $T$ is the number of
data points in the training dataset.

The training seeks to learn a model with output $y(n) \in
\setR^{N_y}$, where $y(n)$ matches $y^{target}(n)$ as close as
possible, by means of the minimization of an error measure
$E(y,y^{target})$ such that it also generalizes well to unseen data
\citep{Lukose2012}.
%